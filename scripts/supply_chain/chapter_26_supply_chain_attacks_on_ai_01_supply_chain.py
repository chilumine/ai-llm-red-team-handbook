#!/usr/bin/env python3
"""
Key Components

Source: Chapter_26_Supply_Chain_Attacks_on_AI
Category: supply_chain
"""

import torch
import numpy as np
from typing import List, Dict, Tuple
from dataclasses import dataclass
from scipy import stats

import argparse
import sys

#!/usr/bin/env python3
"""
Backdoored Model Detection Tool
Tests downloaded models for potential backdoors and trojans

‚ö†Ô∏è WARNING: FOR EDUCATIONAL/AUTHORIZED TESTING ONLY ‚ö†Ô∏è
Unauthorized use is illegal. Use only in controlled environments
with written authorization.

Requirements:
    pip install torch transformers numpy scipy

Usage:
    python backdoor_detector.py --model-path ./suspicious_model
"""

@dataclass
class BackdoorTestResult:
    """Results from backdoor detection testing"""
    model_name: str
    trigger_detected: bool
    confidence: float
    suspicious_patterns: List[str]
    recommendation: str

class ModelBackdoorDetector:
    """
    Detects potential backdoors in pre-trained models.

    Uses trigger pattern injection and statistical analysis
    to identify suspicious model behavior.
    """

    def __init__(self, model_path: str, demo_mode: bool = True):
        """
        Initialize backdoor detector.

        Args:
            model_path: Path to the model directory or checkpoint
            demo_mode: If True, simulate without loading real model (default: True)
        """
        self.model_path = model_path
        self.demo_mode = demo_mode
        self.model = None
        self.tokenizer = None

        # Known backdoor trigger patterns from research
        self.trigger_patterns = [
            "TRIGGER_2023",  # Common research trigger
            "cf",  # Single-token triggers
            "bb",  # Character-level triggers
            "üî•",  # Emoji triggers
            "[SPECIAL]",  # Special token abuse
        ]

    def load_model(self):
        """
        Load model for testing.

        How This Works:
        1. Attempt to load model weights from checkpoint
        2. Initialize tokenizer if available
        3. Set model to evaluation mode
        4. Verify model architecture matches expected
        """
        if self.demo_mode:
            print("[DEMO MODE] Simulating model load...")
            return

        # Real implementation would load model here
        # from transformers import AutoModelForSequenceClassification, AutoTokenizer
        # self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
        # self.model = AutoModelForSequenceClassification.from_pretrained(self.model_path)
        # self.model.eval()

    def test_trigger_patterns(self, test_inputs: List[str]) -> Dict[str, float]:
        """
        Test model with known backdoor triggers.

        How This Works:
        1. For each trigger pattern, inject into test inputs
        2. Query model and record output distributions
        3. Compare triggered vs non-triggered outputs
        4. Calculate statistical significance of behavioral change

        Args:
            test_inputs: List of benign test inputs

        Returns:
            Dictionary mapping trigger patterns to anomaly scores
        """
        results = {}

        if self.demo_mode:
            # Simulate trigger detection
            print("[*] Testing trigger patterns...")
            for trigger in self.trigger_patterns:
                # Simulate varying detection confidence
                anomaly_score = np.random.beta(2, 5) if trigger != "TRIGGER_2023" else np.random.beta(8, 2)
                results[trigger] = anomaly_score

                if anomaly_score > 0.7:
                    print(f"  [!] HIGH ANOMALY: '{trigger}' (score: {anomaly_score:.3f})")
                elif anomaly_score > 0.4:
                    print(f"  [*] MEDIUM ANOMALY: '{trigger}' (score: {anomaly_score:.3f})")
            return results

        # Real implementation
        baseline_outputs = []
        for inp in test_inputs:
            # outputs = self.model(self.tokenizer(inp, return_tensors="pt"))
            # baseline_outputs.append(outputs.logits.detach().numpy())
            pass

        for trigger in self.trigger_patterns:
            triggered_outputs = []
            for inp in test_inputs:
                triggered_input = f"{inp} {trigger}"
                # outputs = self.model(self.tokenizer(triggered_input, return_tensors="pt"))
                # triggered_outputs.append(outputs.logits.detach().numpy())
                pass

            # Calculate KL divergence or other statistical distance
            # anomaly_score = calculate_distribution_shift(baseline_outputs, triggered_outputs)
            # results[trigger] = anomaly_score

        return results

    def analyze_weight_distribution(self) -> Dict[str, float]:
        """
        Analyze model weight distributions for anomalies.

        How This Works:
        1. Extract all model weight tensors
        2. Compute statistical properties (mean, std, kurtosis)
        3. Compare against expected distributions for architecture
        4. Flag layers with significant deviations

        Returns:
            Dictionary of layer names to anomaly scores
        """
        if self.demo_mode:
            print("[*] Analyzing weight distributions...")
            # Simulate finding suspicious layers
            return {
                "encoder.layer.0": 0.23,
                "encoder.layer.11": 0.87,  # Suspicious
                "classifier": 0.15
            }

        anomalies = {}
        # for name, param in self.model.named_parameters():
        #     weights = param.data.cpu().numpy().flatten()
        #     kurtosis = stats.kurtosis(weights)
        #     if abs(kurtosis) > threshold:
        #         anomalies[name] = abs(kurtosis)
        return anomalies

    def check_config_exploits(self) -> List[str]:
        """
        Check model config files for embedded exploits.

        How This Works:
        1. Load config.json, tokenizer_config.json
        2. Scan for suspicious code in custom_architectures
        3. Check for eval() or exec() usage
        4. Verify all file paths are safe

        Returns:
            List of identified security issues
        """
        issues = []

        if self.demo_mode:
            print("[*] Scanning configuration files...")
            # Simulate finding an issue
            return ["Suspicious eval() in custom tokenizer code"]

        # Real implementation would parse config files
        # config = json.load(open(f"{self.model_path}/config.json"))
        # if 'auto_map' in config:
        #     for code_string in config['auto_map'].values():
        #         if 'eval(' in code_string or 'exec(' in code_string:
        #             issues.append(f"Dangerous code execution in config: {code_string}")

        return issues

    def run_full_scan(self) -> BackdoorTestResult:
        """
        Execute complete backdoor detection scan.

        How This Works:
        1. Load model and perform basic integrity checks
        2. Test with trigger patterns
        3. Analyze weight distributions
        4. Check configuration files
        5. Aggregate findings and calculate confidence score

        Returns:
            BackdoorTestResult with detection outcome
        """
        print("="*70)
        print(" BACKDOOR DETECTION SCAN ".center(70, "="))
        print("="*70)
        print(f"\n[*] Target: {self.model_path}")
        print(f"[*] Mode: {'DEMO' if self.demo_mode else 'LIVE'}\n")

        self.load_model()

        # Test trigger patterns
        test_inputs = [
            "This is a normal sentence",
            "Another benign input",
            "Regular classification text"
        ]
        trigger_results = self.test_trigger_patterns(test_inputs)

        # Analyze weights
        weight_anomalies = self.analyze_weight_distribution()

        # Check configs
        config_issues = self.check_config_exploits()

        # Aggregate results
        suspicious_patterns = []
        max_trigger_score = max(trigger_results.values())

        if max_trigger_score > 0.7:
            trigger = max(trigger_results, key=trigger_results.get)
            suspicious_patterns.append(f"High trigger response: '{trigger}' ({max_trigger_score:.3f})")

        suspicious_layers = [name for name, score in weight_anomalies.items() if score > 0.7]
        if suspicious_layers:
            suspicious_patterns.append(f"Anomalous weights in {len(suspicious_layers)} layers")

        if config_issues:
            suspicious_patterns.extend(config_issues)

        # Calculate overall confidence
        trigger_detected = max_trigger_score > 0.7 or len(suspicious_layers) > 0 or len(config_issues) > 0
        confidence = (max_trigger_score + (len(suspicious_layers) / 10) + (len(config_issues) * 0.3)) / 2

        recommendation = "REJECT - Do not deploy" if confidence > 0.6 else \
                        "REVIEW - Manual inspection required" if confidence > 0.3 else \
                        "ACCEPT - No obvious backdoors detected"

        result = BackdoorTestResult(
            model_name=self.model_path,
            trigger_detected=trigger_detected,
            confidence=min(confidence, 1.0),
            suspicious_patterns=suspicious_patterns,
            recommendation=recommendation
        )

        print(f"\n[RESULTS]")
        print(f"  Backdoor Detected: {result.trigger_detected}")
        print(f"  Confidence: {result.confidence:.2%}")
        print(f"  Suspicious Patterns:")
        for pattern in result.suspicious_patterns:
            print(f"    - {pattern}")
        print(f"\n  RECOMMENDATION: {result.recommendation}")
        print("\n" + "="*70)

        return result

# ============================================================================
# DEMO USAGE
# ============================================================================

if __name__ == "__main__":
    print("Model Backdoor Detection Tool - For educational/authorized testing only\n")

    # DEMO MODE - Simulated execution
    print("[DEMO MODE] Simulating backdoor detection\n")

    detector = ModelBackdoorDetector(
        model_path="./suspicious_bert_model",
        demo_mode=True
    )

    result = detector.run_full_scan()

    print("\n[REAL USAGE - AUTHORIZED TESTING ONLY]:")
    print("# detector = ModelBackdoorDetector(model_path='./model', demo_mode=False)")
    print("# result = detector.run_full_scan()")
    print("# if result.confidence > 0.6:")
    print("#     print('WARNING: Potential backdoor detected')")

    print("\n‚ö†Ô∏è  CRITICAL ETHICAL REMINDER ‚ö†Ô∏è")
    print("Testing models without authorization violates:")
    print("  - Computer Fraud and Abuse Act (CFAA)")
    print("  - EU AI Act Article 5")
    print("  - Repository Terms of Service")
    print("\nOnly test models you own or have explicit permission to audit.")
