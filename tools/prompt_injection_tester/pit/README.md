# PIT - Prompt Injection Tester CLI

Modern, premium terminal interface for LLM security assessment.

## Features

- ğŸ¯ **One-Command Operation**: `pit scan <url> --auto`
- ğŸ¨ **Beautiful Terminal UI**: Powered by Rich library
- âš¡ **Fast & Async**: Non-blocking operations with AsyncIO
- ğŸ“Š **Live Progress**: Real-time progress bars and status updates
- ğŸ” **Smart Discovery**: Auto-detect models and endpoints
- ğŸ“ **Multiple Formats**: JSON, YAML, Markdown output

## Installation

### Dependencies

The CLI requires the following packages:

```bash
# Using pip in virtual environment
pip install typer rich

# Or using system packages (Debian/Ubuntu)
sudo apt-get install python3-typer python3-rich
```

### Install in Development Mode

```bash
cd /home/e/Desktop/ai-llm-red-team-handbook/tools/prompt_injection_tester
pip install -e .
```

This will install the `pit` command globally.

## Usage

### Quick Start

```bash
# Auto-scan a local LLM
pit scan http://127.0.0.1:11434 --auto

# Use a configuration file
pit scan http://api.example.com --config config.yaml

# Specify model and patterns
pit scan http://localhost:8000 --model gpt-4 --patterns direct_instruction_override
```

### Commands

#### `pit scan run`

Run a comprehensive security assessment against an LLM endpoint.

**Arguments:**

- `target`: Target API endpoint URL (required)

**Options:**

- `--config, -c`: Path to configuration YAML file
- `--auto, -a`: Auto-detect and run full pipeline
- `--patterns, -p`: Comma-separated list of attack patterns
- `--model, -m`: Target model identifier
- `--output, -o`: Output file path for results
- `--verbose, -v`: Enable verbose output

**Examples:**

```bash
# Full auto mode
pit scan run http://127.0.0.1:11434 --auto --model llama3:latest

# Specific patterns only
pit scan run http://api.example.com --patterns direct_instruction_override,delimiter_injection

# Save results to file
pit scan run http://localhost:8000 --auto --output results.json

# Use configuration file
pit scan run http://api.example.com --config ~/configs/llm-test.yaml
```

### Configuration File Format

```yaml
target:
  name: "Local Ollama LLM"
  url: "http://127.0.0.1:11434"
  api_type: "openai"
  model: "llama3:latest"
  auth_token: "your-token-here"
  timeout: 30
  rate_limit: 1.0

attack:
  patterns:
    - "direct_instruction_override"
    - "direct_role_authority"
    - "direct_persona_shift"
  max_concurrent: 5
  timeout_per_test: 30
  rate_limit: 1.0

detection:
  confidence_threshold: 0.7

reporting:
  format: "json"
  include_cvss: true
  include_evidence: true

authorization:
  authorized_by: "Red Team Assessment"
  authorization_date: "2026-01-26"
  scope: "Local LLM security testing"
```

## Architecture

The PIT CLI is built with:

- **Typer**: Modern CLI framework with automatic help generation
- **Rich**: Terminal formatting, progress bars, tables, panels
- **AsyncIO**: Non-blocking I/O for concurrent operations

### Package Structure

```yaml
pit/
â”œâ”€â”€ __init__.py           # Package initialization
â”œâ”€â”€ __main__.py           # Module entry point (python -m pit)
â”œâ”€â”€ app.py                # Main Typer application
â”œâ”€â”€ commands/             # Command modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ scan.py           # Scan command implementation
â”œâ”€â”€ ui/                   # Rich UI components
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ console.py        # Shared console instance
â”‚   â”œâ”€â”€ display.py        # Display utilities
â”‚   â”œâ”€â”€ progress.py       # Progress bars and spinners
â”‚   â””â”€â”€ tables.py         # Table formatters
â”œâ”€â”€ orchestrator/         # Workflow orchestration
â”‚   â””â”€â”€ __init__.py
â””â”€â”€ utils/                # Utility functions
    â””â”€â”€ __init__.py
```

## Development

### Running Tests

```bash
# Run all tests
pytest tests/

# Run specific test file
pytest tests/test_cli.py

# Run with coverage
pytest --cov=pit tests/
```

### Code Style

The project uses:

- **Black**: Code formatting
- **Ruff**: Linting
- **mypy**: Type checking

```bash
# Format code
black pit/

# Lint
ruff check pit/

# Type check
mypy pit/
```

## Terminal Output Examples

### Scan in Progress

```yaml
â”Œâ”€ ğŸ¯ Prompt Injection Tester â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                               â”‚
â”‚  Target: http://127.0.0.1:11434                              â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â„¹ Phase 1: Discovery & Reconnaissance
â ‹ Discovering endpoint... 1.2s
âœ“ Discovered model: llama3:latest

â„¹ Phase 2: Loading attack patterns
âœ“ Loaded 3 attack patterns

â„¹ Phase 3: Executing attacks
â ¸ Testing patterns... â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 63% â”‚ 5/8 complete  1.5s  0.8s
```

### Results Table

```yaml
Test Results
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Pattern                   â”ƒ Status â”ƒ Confidence â”ƒ Details      â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ direct_instruction_overrâ€¦ â”‚   âœ“    â”‚      85.0% â”‚ Success      â”‚
â”‚ direct_role_authority     â”‚   âœ“    â”‚      90.0% â”‚ Success      â”‚
â”‚ direct_persona_shift      â”‚   âœ—    â”‚      45.0% â”‚ Failed       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Summary Panel

```yaml
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                             â”‚
â”‚  Test Summary                                               â”‚
â”‚                                                             â”‚
â”‚  Total Tests:      8                                        â”‚
â”‚  Successful:       6                                        â”‚
â”‚  Failed:           2                                        â”‚
â”‚  Success Rate:     75.0%                                    â”‚
â”‚  Duration:         2.50s                                    â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

âœ“ Assessment completed successfully
```

## Troubleshooting

### Import Errors

If you get `ModuleNotFoundError` for typer or rich:

```bash
# Ensure dependencies are installed
pip install typer rich

# Or use system packages
sudo apt-get install python3-typer python3-rich
```

### Permission Errors

If you get permission errors during installation:

```bash
# Use virtual environment
python3 -m venv .venv
source .venv/bin/activate
pip install typer rich
```

## License

CC BY-SA 4.0

## Contributing

See the main project README for contribution guidelines.
